
\documentclass[runningheads]{llncs}
\usepackage[show]{ed}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{stex-logo}
\usepackage{paralist}
\usepackage[hidelinks]{hyperref}
\usepackage{orcidlink}
%\usepackage{lststex} debug that the gray is not solid
\usepackage{listings}
\definecolor{codegray}{rgb}{0.9,0.9,0.9}
\lstset{basicstyle=\sf,columns=fullflexible,backgroundcolor = \color{codegray}}
\lstset{numberstyle=\tiny}
\lstset{language={[LaTeX]TeX}}
\usepackage[style=alphabetic,hyperref=auto,defernumbers=true,backend=bibtex,firstinits=true,maxbibnames=9,maxcitenames=3,isbn=false]{biblatex}
\usepackage{xurl}
% \addbibresource{kwarcpubs.bib}
% \addbibresource{extpubs.bib}
% \addbibresource{kwarccrossrefs.bib}
% \addbibresource{extcrossrefs.bib}
\addbibresource{kwarc.bib}
\addbibresource{otherrefs.bib}
\usepackage[noabbrev]{cleveref}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usetikzlibrary{arrows}
\usetikzlibrary{docicon}
\usepackage{soul}
% \def\edited#1{\hl{#1}}
\def\edited#1{#1}


\newcommand\ALeA{\textsf{ALeA}\xspace}
\newcommand\SMGloM{\textsf{SMGloM}\xspace}
\newcommand\snify{\textsf{snify}\xspace}
\newcommand\defianno{\textsf{defianno}\xspace}
\newcommand\FTML{\textsf{FTML}\xspace}
\newcommand\WOIDE{\textsf{WOIDE}\xspace}
\def\llangle{\langle\kern-.2em\langle}
\def\rrangle{\rangle\kern-.2em\rangle}
% \def\flams{\textsf{FLAMS}\xspace}

\lstset{literate=--1}

\title{
        Semantic Authoring in a Flexiformal Context --- Bulk Annotation of Rigorous Documents
}
\titlerunning{
    Semantic Authoring in a Flexiformal Context \textellipsis
}

\author{Michael Kohlhase\orcidlink{0000-0002-9859-6337} \and Jan Frederik Schaefer\orcidlink{0000-0003-2545-4626}}

\institute{Computer Science, FAU Erlangen N\"urnberg, Germany}
\begin{document}
\maketitle
\begin{abstract}
  Semantic annotations enable services that make the knowledge in documents more
  accessible.  In the case of \ALeA, an adaptive learning assistant we use as a case study, they
  enable, for example, guided tours and practice problems tailored to the learner's
  competencies.  However, the cost of semantic authoring is substantial and tool support
  is limited.

  In this paper we discuss the nature of the semantic authoring task and argue that,
  while sharing aspects of both classical (informal) authoring and formal authoring, it is
  a task of its own that needs special editing support facilities.  We present and
  evaluate a simple utility that supports the process of annotating technical terms with
  semantic references to a flexiformal content commons and increases annotation
  productivity by almost an order of magnitude.
\end{abstract}

\keywords{\edited{semantic authoring} \and flexiformality \and annotation \and \snify \and \sTeX \and \ALeA}
% \setcounter{tocdepth}{3}
% \tableofcontents

\section{Introduction}
Arguably, handling large document collections is a key factor in our knowledge-driven
society and economy.  There are two main contenders for machine support in this area:
symbolic/logic-based technologies and statistical/machine-learning-based AI, e.g.\ via
LLMs or chatbots.  They have complementary strengths and challenges: ML-based approaches
can be trained on all data of the Internet, but face challenges in precision and
explainability.  Symbolic technologies, on the other hand, offer precision and
explainability, but face scalability challenges -- in particular because of the cost of
semantic authoring, which we discuss in this paper.

As a case study, we will use adaptive learning assistants for STEM subjects in higher
education, concretely the \ALeA system \cite{BerBetChu:lssmkm23}.  We contend that
machine-learning based technologies are largely unsuitable for this task -- at least until
they reach the precision and explainability required by the ethics of
teaching\footnote{Incidentally, the AI Act of the European Union \cite{EUAIAct:on}
    classifies education as a high-risk area and thus mandates high requirements of
    explainability, accountability, and human oversight for educational AI applications.}
-- while the scope of specific domains is just narrow enough that symbolic approaches are
viable.  Conventional (\LaTeX-based) lecture materials can essentially be directly
imported into \ALeA, but its ability to generate learner-adaptive learning objects --
instrumented with learning support interactions -- depends on semantic annotations.  These
can be embedded in \LaTeX\ documents using the \sTeX
package~\cite{MueKo:sdstex22,sTeX:github:on}.  In this paper, we will mostly focus on
annotating terms with the semantic concepts they refer to, but \sTeX supports a much wider
range of annotations, such as annotating problems with the learning prerequisites
and objectives or annotating the defining phrase (definiens) in concept definitions,
and could, in principle, even be used for full formalization.  As \sTeX supports the
spectrum from informal (un-annotated) to fully formalized content, we say that \sTeX
content is \textbf{flexiformal}.

The \textbf{\sTeX/\ALeA content commons} consists of \SMGloM~\cite{GinIanJuc:spsttom16},
a glossary acting as the domain model for semantic annotations,
as well as re-usable lecture materials such as lecture slides and problems.
% -- mostly from the area of computer science.
The sheer size
of the commons is one of the key challenges for authors: \SMGloM contains definitions of over
5400 concepts in multiple languages, created and maintained by over 30
contributors.  Additionally, there are roughly 4900
semantically annotated lecture slides and 4000 problems
-- mostly from the area of computer science.
% grep -R --include='*.tex' -F -e '\begin{frame}' | wc -l


\paragraph{Overview and Contribution}
In this paper, we argue that semantic/flexiformal authoring shares aspects with both informal and fully formal authoring,
but is nevertheless a distinct task requiring special tool support (\Cref{sec:semauth}).
Using \sTeX/\ALeA as an exemplary case study,
we will provide an overview of the authoring support (\Cref{sec:tools})
and then describe and evaluate one new, concrete tool, \snify, that supports the annotation
of terms with the semantic concepts they refer to (\Cref{sec:snify}).
Afterwards, we discuss on-going work to integrate \snify better
into the \sTeX/\ALeA ecosystem and content commons (\Cref{sec:scaling}).
The paper ends with a brief discussion of related work (\Cref{sec:relwork}) and the conclusion (\Cref{sec:conclusion}).


\paragraph{Acknowledgments}
The work reported in this article was conducted as part of the VoLL-KI project (see
\url{https://voll-ki.de}) funded by the German Research/Education Ministry (BMFTR) under
grant 16DHBKI089.

\section{Semantic Authoring in a Content Commons}\label{sec:semauth}

Before we discuss semantic authoring in general, let us introduce a concrete example from
the \sTeX/\ALeA world.
% We will use it to sharpen our intuitions of the challenges involved and to ground the
% solution we propose.

\begin{figure}\centering
  \fbox{\includegraphics[width=10cm,trim=0.8cm 0 0 0]{../img/minimax-remarks.pdf}}
  \caption{An unannotated learning object from an AI lecture}\label{fig:lo}
\end{figure}

\subsection{Running Example: Annotating ``terminal''}
With \sTeX, authors can add semantic references to technical terms,
which enables straightforward learning support services like
showing the definition of a term, but also informs the system
about the content/prerequisites of a learning object.
Adding these references is a key practical problem when converting presentational {\LaTeX}
course materials into semantic documents.

Take for instance a learning object like the paragraph in \Cref{fig:lo},
    where we want to annotate the
term ``terminal'' with the (semantic) concept of ``a goal state of a search problem''. The
latter can either be defined earlier in the course or in the domain model (or
both). Concretely this is about converting the unannotated {\LaTeX} string
\begin{lstlisting}[numbers=left,firstnumber=3,
% caption=The unannotated {\LaTeX} sources of \Cref{fig:lo},label=lst:los
]
\blue{Note:} Depth-limited minimax requires an evaluation for every
cut-off state $s$. If $s$ is terminal, we use its utility,
and otherwise an estimate.
\end{lstlisting}
into the partially annotated 
\begin{lstlisting}[morekeywords={sr,importmodule},numbers=left,
% caption=Annotating ``terminal'' in \Cref{lst:los},label=lst:losa
]
\importmodule[smglom/search]{mod?search-problem}
[...]
\blue{Note:} Depth-limited minimax requires an evaluation for every
cut-off state $s$. If $s$ is \sr{goal state}{terminal}, we use its utility,
and otherwise an estimate.
\end{lstlisting}
where the author has annotated the word ``terminal'' with the concept it refers to via the
\sTeX macro
\lstinline[mathescape]|\sr{$\llangle \mathit{symbol} \rrangle$}{$\llangle \mathit{verbalization}\rrangle$}|.
In \sTeX, concepts are represented by \textbf{symbols}, which are identified globally and
unambiguously by a URI that includes a \textbf{namespace}, a \textbf{module name}, and a
\textbf{symbol name}.  A \textbf{verbalization} is the natural language rendering of a
concept.  Verbalizations are, of course, language-dependent and can vary between
subdomains and communities of practice, even if the underlying symbol is the same.  In our
example, the verbalization ``terminal [state]'' was annotated with the symbol with name
\lstinline|goal state|. It comes from the module with name \lstinline|search-problem| in
the namespace \lstinline|smglom/search/mod| imported via the \lstinline|\importmodule|
macro in line 1. As the symbol name \lstinline|goal state| is unique in the imports here
it suffices to use a relative URI in the \lstinline|\sr| annotation. If the symbol name
and the desired verbalization coincide (as they often do in English), the short-hand macro
\lstinline|\sn| can be used (e.g. as \lstinline|\sn{utility}| for ``utility'').
% in \Cref{lst:losa}).


The annotation refers to a definition in the domain model module shown in
\Cref{fig:state-space}.  The \lstinline|\definame| and \lstinline|\definiendum| in lines
5--6 introduce the verbalizations ``goal state'' and ``terminal state'' for the
\lstinline|goal state| symbol introduced by the \lstinline|\symdecl*| declaration in
line 3.

\begin{figure}\centering
  \fbox{\includegraphics[width=12cm]{../img/search-problem.en.pdf}}
\begin{lstlisting}[morekeywords={definame,symdecl,definiendum},numbers=left,
escapechar=!]
\begin{smodule}[title=Search Problem]{search-problem}
[... some imports ...]
\symdecl*{goal state}
\begin{sdefinition}
  [...] Certain \sns{state} are [...] \definame[post=s]{goal state} [...]
  (also called \definiendum{goal state}{terminal states}).
\end{sdefinition}
\end{smodule}
\end{lstlisting}
  \caption{Simplified definition of ``goal state'' from the domain model}\label{fig:state-space}
\end{figure}

\subsection{Annotation Workflows and Requirements}\label{sec:workflows}

To annotate the word ``terminal'' in the document above, the author has to 
\begin{inparaenum}[\em i\rm)]
\item be aware that it is a
technical term, 
\item that the module from \Cref{fig:state-space} exists,
\item know the symbol URI, 
\item manage redundancy of the imports -- i.e. only adding the \lstinline|\importmodule|
  directive if it is not already (recursively) implied and possibly removing directives
  that become redundant by the new one.
\end{inparaenum}
A rather daunting task for a lecture with 50,000 -- 100,000 words in notes and slides and ca. 10--15\,\% of them technical terms.\footnote{
        Estimates based on some of our own lectures materials
        ranging from a small introductory lecture (ca. 30,000 words)
        to a large two-semester lecture on AI (ca. 140,000 words).
        The 10--15\,\% estimate appears to be relatively consistent across our lectures
        but depends on the author's intuition about what a technical term is.
        Other lecturers or lectures on other topics may have very different values.
}

While annotation needs to happen during authoring and curation of documents, the majority
of annotation tasks in practice concern the semantic augmentation of existing documents
(\textbf{bulk annotation}) with respect to an existing semantic domain model. In any case,
it is probably useful to separate the roles of author and semantic annotator conceptually,
even though they may well be the same person in practice.

The most important resource for annotators is a verbalization to symbol URI mapping -- we
call it the \textbf{annotation catalog} -- either implicitly in the mind of the annotator
or explicitly as a technical artifact, which allows to
\begin{inparaenum}[\em i\rm)]
\item identify the known technical terms,
\item to map them to the corresponding symbols, and 
\item manage the necessary import directives.
\end{inparaenum}
This catalog can be harvested from the \textbf{formal declarations}
(\lstinline|\definiendum|, \lstinline|\sr|, \lstinline[language={}]|\begin|/\lstinline[language={}]|\end{smodule}| and
\lstinline|\importmodule|, etc.) of the domain model and existing documents in the
semantic corpus.

The annotation catalog is naturally language-specific.
\sTeX already supports multilinguality:
\begin{itemize}
\item Documents have explicit language annotations (usually in form of the file extension
  \lstinline[mathescape]|.$\llangle lang\rrangle$.tex| where $\llangle lang\rrangle$ is the
  ISO-639 language identifier).
\item Modules in the domain model include translations that share the formal
    declarations with another module.
        This induces a ``translation relation'' for verbalizations
        in different languages with the same symbol URI.
\end{itemize}
In practice, the annotation catalog for languages other than English is often inadequate as
\begin{enumerate}[\em i\rm)]
    \item the domain model may not have translations in that language -- especially for the ``smaller'' languages, and
    \item symbol names, usually English words, cannot be used as verbalizations.
\end{enumerate}
In this situation, we usually have to invest in
establishing a suitable catalog -- e.g. by providing translations (or at least dictionary
information) in the domain model or
delineating modules and annotating definienda that the
domain model does not cover in the documents to be annotated.


\subsection{Semantic Authoring as a Distinct Task}
We contend that semantic authoring is a task that is distinct from traditional
(informal) authoring and authoring fully formal corpora (e.g. programs or formalizations),
while sharing some of the aspects of either.

\paragraph{Traditional, informal authoring}
In traditional authoring, e.g. of a scientific article or a textbook, the domain model --
a highly structured model of the domain of discourse -- is in the authors' heads, and is
partially verbalized e.g. in the preliminaries section or the main sections of the
respective article. The content is informal (usually a natural language like English
augmented with technical jargon, formulae, tables, and diagrams) and designed for
processing via the brain of a colleague or student -- in any case, a human who shares a
canonical part of the domain model with the authors. Consequently a large part of the
authoring challenge lies in predicting the state and extent of the domain model of the
reader and creating text that bridges the gap between the reader's domain model and the
payload content of the article -- the knowledge the article or textbook intends to impart
-- ideally using the technical vocabulary the reader is familiar with to reduce this
gap. Tool support for traditional authoring is minimal and usually restricted to
spell/grammar-checkers and online thesauri if we disregard LLM-based writing/formulation
support for now.
In our experience, LLM-based authoring support is less helpful in technical documents
than in everyday writing.

\paragraph{Formal Authoring}
In programming, the content commons is given formally in the form of the libraries of the
respective programming language. Correspondingly the authored content -- a program -- is
also formal and intended for processing by a computer, for which the formality is a
prerequisite. Formal authoring is usually supported by an integrated development
environment (IDE) that makes use of the fully formal content and offers services like
semantic highlighting, tab completion, jumping to variable declarations, function
definitions, and interface definitions, as well as refactoring support. All of these rely
on a syntactic/semantic analysis of the formal content. The situation is very similar for
authoring of formal knowledge, e.g. in program verification or the formalization of
mathematics.

\paragraph{Semantic Authoring}
In semantic authoring, e.g. the \sTeX sources of educational content for the \ALeA system
presented above, the base of the text is classical narrative text to which most of the
constraints of classical authoring apply.
The exception is the tailoring to the competencies of the assumed reader
that traditionally happens during authoring, but which can (partially) be automated by systems like
\ALeA. Note that it is not a restriction that \ALeA is initially targeted to tertiary
education; the underlying mechanisms generalize to all scientific communication and
technical documents. After all, the purpose is to transfer information (instruct), even if
the roles of instructor and instructee are not predetermined by academic seniority.

In addition to the base text, semantic authoring also has to provide the semantic
annotations that drive the personalization and interaction downstream. Techniques from formal
authoring could help here, but in contrast to formal authoring, we cannot just trigger the
IDE functionality by the formal syntax, as the authored content is -- so far -- informal
and the content commons (the target of the annotations) is itself only flexiformal. In a
way, we need a local flexiformalization workflow that helps authors bridge the
informal-formal gap.
We have experimented with LLM-based generation of annotated (\sTeX) content,
but the results have been disappointing so far.


A distinctive feature of semantic authoring that is not apparent at the first glance is
that the added investment in annotation moves the sweet spot in the re-use/customize
trade-off towards sharing and interoperability. In our experience sufficiently annotated
documents can be automatically customized if the semantics has multiple
notations/verbalizations/presentations. Consider for instance course materials on complex
arithmetic, which become re-usable across the electrical engineering/mathematics border if
the imaginary unit in formulae is represented e.g. by a semantic macro
\lstinline|\imaginaryunit|, which can alternatively \edited{be presented as} $i$ (for uses in maths) or
$j$ (for EE uses). As a consequence semantic authoring is more geared towards creating
reusable/shareable document fragments organized in central or federated repositories and
hubs. This is quite similar to the formal authoring arena, where we see a tendency towards
massively shared math libraries in formal systems and package/library hubs for programming
libraries. Semantic authoring support systems should therefore facilitate the
logistics of using and curating shared libraries.

\section{Tool Support for Semantic Authoring in \sTeX/\ALeA}\label{sec:tools}

Semantic authoring can ``inherit'' tool support from both informal and formal authoring.
In the case of \sTeX, we can re-use spell checkers for traditional informal authoring.
There also is prototypical support for authoring \ALeA content in Microsoft Word with
the \WOIDE plugin \cite{KohKoh:woide24}, which allows authors to use Word's spell
checker and grammar checker.

On the formal side, there is a custom VSCode IDE plugin for \sTeX \cite{sTeX-IDE:git} that
analyzes annotations on the fly, displays the underlying reference semantics, reports
errors and redundancies, and even offers a concept search interface.  But such features
only support existing annotations and are, therefore, of limited use when creating new
annotations, which arguably is the most time consuming aspect of semantic authoring.
% \ednote{also: SPARQL API}

How the annotation process can be supported depends on the particular semantic framework,
the specific annotations we want to support, and the semantics embedded in the content
commons.  In \sTeX/\ALeA, we primarily annotate formulae and technical terms with pointers
to their definienda.  A tool for supporting semantic authoring of formulae via notations
has been presented by Vre{\v{c}}ar et al.\ in~\cite{VreWelKam:tsmmdui24},
\edited{so we disregard this important aspect here.
The tool} uses a grammar to parse informal, presentation-focused formulae (e.g.\
\lstinline[keywordstyle={}]|A\times B|) into abstract syntax trees based on semantic
macros defined in \sTeX, which can then replace the original formula (e.g.\
\lstinline|\cart{A,B}| for the Cartesian product of sets).  In the case of ambiguity,
authors can pick the correct reading in a graphical user interface that visualizes the
different abstract syntax trees.

Efficient support for the annotation of technical terms in natural language
has been elusive so far -- we will address this in the next section.


\section{The \snify System}\label{sec:snify}

The \snify\footnote{ From ``\texttt{\textbackslash sn}-ify''; ``\texttt{\textbackslash
    sn}'' is a frequently used \sTeX short-hand macro for term annotations.}  utility
\cite{stextools:git} is a simple command-line tool that creates a catalog of
symbol-verbalization pairs (see also \Cref{sec:workflows}), analyzes \sTeX source files, and then steps through all word
occurrences in the document that match a verbalization in the catalog.  Matching is based
on word stems using off-the-shelf stemmers, which allows us to e.g.\ match ``utilities''
with ``utility''.  For each matched word, the user is presented with an annotation choice
and interactions that allow to fine-tune the wordwise annotation workflow.  The interface
is inspired by traditional spell checkers like \lstinline|ispell|~\cite{ispellman}.  \Cref{fig:snify} shows
a typical situation.

\begin{figure}[ht]
  \setlength{\fboxsep}{0pt}
  \fbox{\includegraphics[width=12cm,trim={0 3cm 0 0},clip]{../img/snify}}
  \caption{\snify in Action: Annotating the word ``terminal''}\label{fig:snify}
\end{figure}

The annotator can choose an annotation by typing the corresponding choice number,
in this case \lstinline|0|.
The little green check-mark indicates that the relevant module is already imported.
Otherwise \snify would offer the annotator the choice to import it -- if we are
in a module context -- or to add a \lstinline|\usemodule| directive, either in the
local environment or at the top-level.

To skip the word, the user can type \lstinline|s|;
\lstinline|s!| skips it until the end of the file, and \lstinline|S|
will skip it in future runs as well by appending a \snify-specific comment to the file.
There are numerous other commands, e.g.\ to adjust the selection,
search for alternative annotation targets,
or view/edit the file introducing one of the choices.

\snify is usually called on a set of files -- e.g. in a directory or math archive -- which
together form a \textbf{session}, which can be interrupted and resumed without having to
re-do all the \lstinline|s| skips. Alternatively, \snify can be called e.g. on the
top-level lecture notes; then the session consists of all (transitively) included
files. Another productivity feature is a focus mode that can be used to only annotate
a particular word in the rest of the file, session, or the whole \sTeX/\ALeA corpus before
resuming regular annotation. This reduces the cognitive overhead from switching between
different words and symbol lists.
% Of course the include management still needs to be done locally.

\paragraph{\defianno: Definition Handling as a Pre-process for Multilingual Documents.}
As a companion to \snify, we have \defianno, a prototype tool for annotating definienda in
documents that were originally authored without semantic annotations.  It can be
configured to consider different \LaTeX\ macros to identify definienda (e.g.\
\lstinline[language={}]|\emph| or \lstinline[language={}]|\textbf|), and then steps
through them as \snify does.
For each potential definiendum, the user can then use fuzzy
search to find the corresponding symbol in the domain model
\edited{and replace the original macro with an} \sTeX \edited{definiendum annotation}.
This populates the catalog
with verbalizations relevant to the document, making subsequent \snify runs more
effective, especially for documents in languages that are not well covered by the domain
model (i.e.\ not English).

% \newpage
\paragraph{Evaluation}
% \section{Practical Evaluation and Future Work}\label{sec:eval}
In our experience, \snify's step-through workflow for annotating term references 
% for symbols from the domain model 
is almost an order of magnitude more efficient than writing the
annotations and imports by hand.\footnote{
        The precise productivity gain depends on the annotator's
        familiarity with the domain model (\edited{searching and identifying}
        the right symbol is time-consuming) but also on other factors like
        the typing speed.
        Measurements are complicated by the fact that
        authors tend to annotate more terms when using \snify.
        We have observed speed-ups over IDE-based annotation
        ranging from 3 (for an expert knowing the domain model by heart)
        to 13 (for a beginner).
        The focus mode further increases productivity
        when considering the overall corpus quality.
}
% , even for annotators who are familiar with the domain model.
Given the sheer number of term annotations (see \Cref{fig:annocounts}),
this results in a considerable productivity gain.
For annotators unfamiliar with the domain model, the unassisted annotation task is
\begin{wrapfigure}{r}{0.4\textwidth}
    \vskip-0.7cm
  \centering
  \includegraphics[width=0.4\textwidth]{../img/annocounts.pdf}
    % \vskip-0.2cm
  \caption{Number of annotated terms in the \sTeX/\ALeA commons over time.}\label{fig:annocounts}
  \vskip-0.7cm
\end{wrapfigure}
almost infeasible, and the average unfamiliarity naturally grows with the domain
model. The symbol disambiguation process -- on average a word induces 4.1 choices
% \ednote{
%     more details? median is 3, for existing annotations, the average is 2.6 and the median 2...
% }
-- is still manageable,
requires considerable concentration and domain knowledge, but little knowledge of the
domain model flexiformalization.

The command-line interface is simple and responsive and gives all the necessary
information in a single glance if the underlying shell area exceeds ca. $80\times 35$
characters. It is very much geared towards annotating existing documents with respect to a
relatively complete -- pre-existing -- domain model, and it seems unlikely that a more
sophisticated UI would add value for this use-case.

For less complete domain models we have to skip too many terms that should ultimately be
annotated and annotation effectiveness suffers. This is currently the case for all
non-English languages in the \sTeX corpus we work with. Coverage in German is only about
half of that of English in the domain model and we can already see the practical effects.  The \defianno tool
can mitigate this partially by providing verbalizations for
the definienda in the document, but it does not help with terms that are not explicitly
defined in the document.

Finally, the current interface is not well-suited for on-the-fly annotation while
authoring.
For that, the underlying information (the symbol/verbalization pairs harvested
from the domain model) can be integrated into any IDE. In fact we plan to do this for the
next version of the \sTeX plugin for VSCode \cite{sTeX-IDE:git}.
For now, the typical workflow of early adopters is to first write the document (e.g.\ a set of quiz questions) without annotations, and then run \snify to annotate the new content.

\section{Scaling \snify to the Flexiformal Content Commons }\label{sec:scaling}
One practical problem with the \snify system as presented so far is that it only harvests
symbol/verbalization pairs from \sTeX files on the local file system. This means that, for
effective annotation, users (have to) download the whole content commons and keep it
updated -- an assumption/requirement that becomes increasingly impractical as the \sTeX
ecosystem grows.

The general situation is that on the one hand there is a constantly growing, shared
\textbf{content commons} of public math archives -- currently 200 with about 14\;000 \sTeX
files -- that are regularly converted to \FTML (\underline{F}lexiformal \underline{T}ext
\underline{M}arkup \underline{L}anguage) and served on \url{https://MathHub.info}. On the
other hand, authors are working on
\begin{compactenum}[\em i\rm)]
\item a set of local working copies of the archives -- with the intention of publishing
  them on MathHub.info -- and their dependencies (they need to be local since
  \textsf{pdflatex} cannot deal with remote files) and
\item possibly a set of private archives (e.g.\ papers under development or exam
  problems/solutions).
\end{compactenum}
This situation is familiar from software development and also from e.g. the \LaTeX{}
ecosystem. Semantic authoring -- and thus the \snify utility -- inherits the requirements
from both.


\edited{To cope with this situation, we are planning to extend the brand-new} \flams
system~\cite{flamsongithub}, \edited{which, among other things, powers} \url{https://MathHub.info},
\edited{so that it provides the data necessary to create a
symbol/verbalization catalog.}
Note that
\flams works directly on \FTML, which is generated from \sTeX input by \textsf{rus\TeX}
and exported from annotated MS Word files via \WOIDE~\cite{KohKoh:woide24}. There are two
more systems that could profit from this: The CPoint system that allows annotating MS
PowerPoint presentations with the \sTeX ontology and can already generate a precursor of
\FTML \cite{Kohlhase:SemanticInteractionDesignDiss:biblatex}, and a semantic extension of
Markdown we are developing as a lightweight alternative to \sTeX; it also transforms into
\FTML instead of plain HTML.

\begin{wrapfigure}r{5.5cm}\vspace*{-2em}
  \begin{tikzpicture}[scale=.7]
    \tikzstyle{doc}=[draw,thick,align=center,color=black,
                  shape=document,minimum width=10mm,minimum height=15mm]
    \tikzstyle{database}=[cylinder,shape border rotate=90,aspect=0.25,draw, 
     cylinder uses custom fill,cylinder body fill=yellow!30,cylinder end fill=yellow!30]
     \tikzstyle{include}=[right hook-angle 45,thick]
     \tikzstyle{includeleft}=[left hook-angle 45,thick]
    \node[doc] (cc) at (0,-1) {\shortstack{Content\\ Commons}};
    \node[doc] (pl) at (5,-1) {\shortstack{Private/\\Local}};
    \node[database] (db) at (3,2.5) {\shortstack{\tiny Verbalization\\Cache}};
    \node[draw] (sn) at (0,4.5) {\snify};
    \node[draw] (id) at (2.5,5) {IDE};
    \node[draw] (wo) at (5,4.5) {\WOIDE};
    \draw[draw,fill=black!10] (-1.3,.5) rectangle ++ (6.4,1); 
    \node (f) at (-0.3,1.0) {\flams};
      \draw[include]  (cc) -- node[left] {\tiny \begin{tabular}{c}RDF\\exp.\end{tabular}} (db);
    \draw[includeleft] (pl) -- node[right] {\tiny linter} (db);
    \draw[->] (db) -- (sn);
    \draw[->] (db) -- (id);
    \draw[->] (db) -- (wo);
    \draw[dashed,thick] (3,1.6)  -- (3,-2);
    \draw[dashed,thick] (3,1.6)  -- (3,-2);
    \draw[includeleft] (pl) to[bend right=10] (wo);
  \end{tikzpicture}
  \caption{Architecture}\label{fig:arch}\vspace*{-1em}
  \vskip-0.5cm
\end{wrapfigure}

In local/private archives, the \snify harvester needs to work directly on the \sTeX
sources.
Currently, \snify uses a custom Python implementation for harvesting the verbalization cache,
but \edited{we are in the late stages of developing a new} \snify version using the
\textbf{\flams linter} -- a simple \sTeX parser that
drives the LSP functionality of the \sTeX IDE plugins for \textsf{VSCode} and \textsf{emacs}.
The analog for this on the \WOIDE side is still under development; it
could be based on \FTML generated by \WOIDE, or \WOIDE could directly export symbol/verbalization
pairs into the \snify verbalization cache.

In this situation, the main functionality of \snify would be to maintain the verbalization cache
and answer front-end queries for verbalizations efficiently so that we can power various
front-ends from that. These include
\begin{compactenum}[\em i\rm)]
\item The traditional text/terminal-based \snify UI for bulk annotation,
\item the \sTeX IDEs in \textsf{VSCode} and \textsf{emacs} that could supply annotation
  support during editing, and 
\item \WOIDE, which can be seen as a semantic authoring IDE based on MS Word.
\end{compactenum}

Note that, implemented this broadly, \snify could provide additional logistic services: If a
pair is chosen in an annotation, \snify can then automatically download the corresponding
archive and its dependencies so that the annotated file can be formatted with \texttt{pdflatex} or
\textsf{rus\TeX} locally.

\section{Related Work}\label{sec:relwork}
The phrase ``semantic authoring'' has been used in the
context of the semantic web with varying meanings
(\cite{khalili2013user} provides an overview of the tool support).
This includes ontology authoring (e.g.\ with \textsf{Prot{\'e}g{\'e}}
\cite{musen2015protege}), which we would classify as
formal authoring, and metadata annotation
(e.g. the title, author, and creation date of a document with the Dublin Core vocabulary~\cite{DublinCore:on}),
which is a somewhat different concern.
A closer match is the annotation of
informal content with references to a domain ontology (e.g.\ \cite{goerz2010adaptation}),
but there are some differences to
our situation with \sTeX/\ALeA: The domain ontology would be a fully formal target --
possibly containing verbalizations -- while in our case the target is flexiformal.
Furthermore, \sTeX has a module system, which requires dependency management during
authoring.
A similar application requiring semantic authoring
are semantic wikis, which enrich regular/syntactic
wikis with semantic information
(see e.g.\ \cite{semmediawiki}).

Another example of formal/informal authoring is code documentation.
Often, the documentation is embedded in the code itself,
e.g.\ in Python docstrings, which may follow a specific
machine-readable format.
IDEs, which are primarily used for (formal) code authoring,
typically provide some support, e.g.\ by generating stubs
and by extending refactoring support
to (the machine-readable parts of) the documentation
like parameter names and types.

\snify's interface is inspired by traditional spell checkers
like \lstinline|ispell|~\cite{ispellman}
that step through documents and suggest corrections for misspelled words,
\edited{which is of course a very different underlying task.}

The bulk annotation workflow for existing informal text is also relevant when creating
machine learning datasets, e.g.\ for named entity recognition.  There are many (often
browser-based) tools for this task; a prominent example is
\textsf{brat}~\cite{brat:on} for text annotation.
\edited{
    Technical documents with tables, complex formulae, etc. cannot
    easily be expressed as sequences of words and require different annotation
    tools like} e.g.\ \textsf{AnnoTize}~\cite{PanzerSchaefer:AnnoTize23}
\edited{
    that supports the annotation of HTML documents containing MathML formulae.
}


In all cases, the setting, solution, and resulting workflows differ from the one presented
in this paper. The WissKI system \cite{goerz2010adaptation} has a rudimentary text edition
facility that allows annotating documents with ontology -- mostly ABox -- references; this
may be the nearest match to our \snify system.

\section{Conclusion}\label{sec:conclusion}
In this paper we have presented the semantic authoring task in a flexiformal context
using the \sTeX corpus and the adaptive learning assistant \ALeA as a concrete example. We
have shown that it differs from both classical authoring and formal authoring in terms of
the necessary system support. As an example of specialized authoring support, we have
presented \snify, a command-line tool that bridges the gap between informal text and
formal/semantic symbols in \sTeX.
\edited{It is open source; the code and documentation are available at}
\cite{stextools:git}.
Given the impact of \snify on the authoring process, we are
currently working on a deeper integration of \snify into the \sTeX ecosystem and content
commons as described in \Cref{sec:scaling}. 


\paragraph{Future Work}
\snify depends on the existence of a comprehensive annotation catalog,
which is a severe limitation for languages other than English.
A potential solution would be to extend \snify with a machine
learning-based tool for identifying unannotated technical terms that are not in the
catalog.  The user could then pick a symbol for the annotation, which would add a
verbalization to the catalog.  In an earlier (pre-\snify) attempt, we used named-entity
recognition (NER) for classification of ``likely annotation candidate words''
\cite{hutterer:msc23}. However, this classification was not precise enough in the
distinction between technical terms and ordinary noun phrases to be practical on
its own.  A combination with \snify's catalog-based approach might change the trade-offs
involved.





\printbibliography
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

% LocalWords:  tbw Aarne Ranta homostem pre hmmm Vre ar al sn ify TODO et Hutter Plivelic
% LocalWords:  lang DHBKI FS Todo FTML lexiformal arkup anguage CICM RDF postprocessing
% LocalWords:  de maths EE CPoint SMD rus WOIDE linter LSP emacs Prot BMBF diesem Bild
% LocalWords:  sollten keine blauen annotierten Wörter sein
